{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilfetz22/audio-digest-hub/blob/main/src/audiobooks/TTS_Generation_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279b3006",
      "metadata": {
        "id": "279b3006"
      },
      "source": [
        "# Optimized TTS Audio Generation for Audio Digest Hub\n",
        "\n",
        "This Google Colab notebook provides GPU-accelerated text-to-speech generation with parallel processing optimizations. It processes text files from Google Drive and generates high-quality audio files using the XTTS v2 model.\n",
        "\n",
        "## Features\n",
        "- üöÄ **GPU Acceleration** - Leverages Colab's T4 GPU for 5-20x faster TTS generation\n",
        "- ‚ö° **Parallel Processing** - Batch processing multiple sentences simultaneously  \n",
        "- üß† **Smart Memory Management** - Automatic GPU memory monitoring and cleanup\n",
        "- üìä **Progress Tracking** - Real-time progress indicators and performance metrics\n",
        "- üîÑ **Error Recovery** - Robust error handling with automatic retries\n",
        "- üìÅ **Google Drive Integration** - Seamless file input/output with Drive\n",
        "\n",
        "## Workflow\n",
        "1. Upload your cleaned text file to Google Drive\n",
        "2. Run all cells to process TTS generation\n",
        "3. Download the generated MP3 from Drive\n",
        "4. Continue with local upload process\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e84c7e6f",
      "metadata": {
        "id": "e84c7e6f"
      },
      "source": [
        "## 1. Setup Environment and Dependencies\n",
        "\n",
        "First, we'll install all required packages and configure the environment for optimal GPU performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d1f6e375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1f6e375",
        "outputId": "d1536b97-f349-4ccf-b2f7-ac8b0cb8afd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing dependencies for optimized TTS generation...\n",
            "Installing coqui-tts...\n",
            "‚úÖ coqui-tts installed successfully\n",
            "Installing pydub...\n",
            "‚úÖ pydub installed successfully\n",
            "Installing torch...\n",
            "‚úÖ torch installed successfully\n",
            "Installing torchaudio...\n",
            "‚úÖ torchaudio installed successfully\n",
            "Installing numpy...\n",
            "‚úÖ numpy installed successfully\n",
            "Installing tqdm...\n",
            "‚úÖ tqdm installed successfully\n",
            "Installing psutil...\n",
            "‚úÖ psutil installed successfully\n",
            "Installing GPUtil...\n",
            "‚úÖ GPUtil installed successfully\n",
            "\n",
            "üéâ All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package with progress indication\"\"\"\n",
        "    print(f\"Installing {package}...\")\n",
        "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "                          capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ {package} installed successfully\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to install {package}: {result.stderr}\")\n",
        "\n",
        "# Install core dependencies\n",
        "packages = [\n",
        "    \"coqui-tts\",\n",
        "    \"pydub\",\n",
        "    \"torch\",\n",
        "    \"torchaudio\",\n",
        "    \"numpy\",\n",
        "    \"tqdm\",\n",
        "    \"psutil\",\n",
        "    \"GPUtil\"\n",
        "]\n",
        "\n",
        "print(\"üîß Installing dependencies for optimized TTS generation...\")\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\nüéâ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4dd85fe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dd85fe0",
        "outputId": "bd5a370f-a183-4191-db85-587945e4f9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Running in Google Colab environment\n",
            "üìö All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# Audio processing\n",
        "from pydub import AudioSegment\n",
        "from TTS.api import TTS\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# GPU monitoring\n",
        "try:\n",
        "    import GPUtil\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è GPUtil not available, GPU monitoring disabled\")\n",
        "\n",
        "# Google Colab specific imports\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "    IN_COLAB = True\n",
        "    print(\"üî¨ Running in Google Colab environment\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üñ•Ô∏è Running in local environment\")\n",
        "\n",
        "# Set up logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üìö All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89c4a3f",
      "metadata": {
        "id": "e89c4a3f"
      },
      "source": [
        "## 2. Configure Google Drive Integration\n",
        "\n",
        "Mount Google Drive and set up file paths for reading input text files and saving generated audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5b6892d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b6892d5",
        "outputId": "44cbbc80-0502-4b41-d239-0a9c744df2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "üìÇ Input folder: /content/drive/MyDrive/TTS_Input\n",
            "üìÇ Output folder: /content/drive/MyDrive/TTS_Output\n",
            "\n",
            "üìÑ Found 1 text file(s):\n",
            "   1. digest_2025-10-03_cleaned.txt (69.8 KB)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "if IN_COLAB:\n",
        "    print(\"üìÅ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "\n",
        "    # Set up drive paths\n",
        "    DRIVE_ROOT = '/content/drive/MyDrive'\n",
        "    INPUT_FOLDER = f'{DRIVE_ROOT}/TTS_Input'\n",
        "    OUTPUT_FOLDER = f'{DRIVE_ROOT}/TTS_Output'\n",
        "else:\n",
        "    # Local development paths\n",
        "    INPUT_FOLDER = './input'\n",
        "    OUTPUT_FOLDER = './output'\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(INPUT_FOLDER, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Input folder: {INPUT_FOLDER}\")\n",
        "print(f\"üìÇ Output folder: {OUTPUT_FOLDER}\")\n",
        "\n",
        "# List available text files\n",
        "input_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith('.txt')]\n",
        "if input_files:\n",
        "    print(f\"\\nüìÑ Found {len(input_files)} text file(s):\")\n",
        "    for i, file in enumerate(input_files, 1):\n",
        "        file_path = os.path.join(INPUT_FOLDER, file)\n",
        "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"   {i}. {file} ({file_size:.1f} KB)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No text files found in input folder!\")\n",
        "    print(\"üìã Please upload your cleaned text file to the TTS_Input folder in Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a847ac",
      "metadata": {
        "id": "90a847ac"
      },
      "source": [
        "## 3. Load and Initialize TTS Model\n",
        "\n",
        "Load the XTTS v2 model with GPU optimization and configure settings for maximum performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7c663605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c663605",
        "outputId": "a3e0334c-d162-4293-98e0-88b8eb6bf910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ GPU Available: Tesla T4\n",
            "üî¢ GPU Count: 1\n",
            "üíæ GPU Memory: 14.7 GB\n",
            "\n",
            "üîß Configuration:\n",
            "   Model: tts_models/multilingual/multi-dataset/xtts_v2\n",
            "   Device: cuda\n",
            "   Default Speaker: Claribel Dervla\n",
            "   Token Limit: 390\n"
          ]
        }
      ],
      "source": [
        "# GPU and device configuration\n",
        "def setup_gpu_environment():\n",
        "    \"\"\"Configure optimal GPU settings for TTS\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "\n",
        "        print(f\"üöÄ GPU Available: {gpu_name}\")\n",
        "        print(f\"üî¢ GPU Count: {gpu_count}\")\n",
        "        print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "        # Configure CUDA for optimal performance\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return device, gpu_name, gpu_memory\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è GPU not available, using CPU (this will be much slower)\")\n",
        "        return \"cpu\", \"CPU\", 0\n",
        "\n",
        "device, gpu_name, gpu_memory = setup_gpu_environment()\n",
        "\n",
        "# Constants (matching your original implementation)\n",
        "TTS_MODEL = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "DEFAULT_SPEAKER = \"Claribel Dervla\"\n",
        "XTTS_TOKEN_LIMIT = 390\n",
        "\n",
        "print(f\"\\nüîß Configuration:\")\n",
        "print(f\"   Model: {TTS_MODEL}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Default Speaker: {DEFAULT_SPEAKER}\")\n",
        "print(f\"   Token Limit: {XTTS_TOKEN_LIMIT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ea85336d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea85336d",
        "outputId": "16442b23-1aca-47f9-eb12-20d363bdae19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading TTS model (this may take a few minutes)...\n",
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
            " | | > y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.87G/1.87G [00:24<00:00, 75.9MiB/s]\n",
            "4.37kiB [00:00, 6.30MiB/s]\n",
            "361kiB [00:00, 24.8MiB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.0/32.0 [00:00<00:00, 59.7kiB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.75M/7.75M [00:00<00:00, 60.8MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TTS model loaded successfully in 48.2 seconds\n",
            "üî• Warming up model with test synthesis...\n",
            "‚úÖ Model warmed up successfully\n",
            "\n",
            "üìä Model Information:\n",
            "   Model name: tts_models/multilingual/multi-dataset/xtts_v2\n",
            "   Device: cuda\n",
            "   Speaker options: Available (using Claribel Dervla)\n",
            "   GPU Memory - Allocated: 1.79 GB, Reserved: 1.84 GB\n"
          ]
        }
      ],
      "source": [
        "# Initialize TTS model with optimization\n",
        "print(\"üîÑ Loading TTS model (this may take a few minutes)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Initialize TTS model\n",
        "    tts_client = TTS(TTS_MODEL).to(device)\n",
        "\n",
        "    # Get tokenizer for validation\n",
        "    tokenizer = tts_client.synthesizer.tts_model.tokenizer\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"‚úÖ TTS model loaded successfully in {load_time:.1f} seconds\")\n",
        "\n",
        "    # Model warming - synthesize a short test sentence for consistent performance\n",
        "    print(\"üî• Warming up model with test synthesis...\")\n",
        "    try:\n",
        "        test_text = \"This is a test to warm up the model.\"\n",
        "        _ = tts_client.tts(text=test_text, language=\"en\", speaker=DEFAULT_SPEAKER)\n",
        "        print(\"‚úÖ Model warmed up successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Model warming failed: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load TTS model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Display model information\n",
        "print(f\"\\nüìä Model Information:\")\n",
        "print(f\"   Model name: {tts_client.model_name}\")\n",
        "print(f\"   Device: {device}\")  # Use the device variable we set earlier\n",
        "print(f\"   Speaker options: Available (using {DEFAULT_SPEAKER})\")\n",
        "\n",
        "# Memory check after model loading\n",
        "if device == \"cuda\":\n",
        "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
        "    print(f\"   GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cdc63b",
      "metadata": {
        "id": "50cdc63b"
      },
      "source": [
        "## 4. Implement Optimized Text Processing Pipeline\n",
        "\n",
        "Create functions for text chunking, validation, and batch preparation with token limit enforcement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fc98e5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc98e5d0",
        "outputId": "94ea7bdc-4b8a-41de-c5f1-dee3c4ec26b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Text processing functions defined successfully\n"
          ]
        }
      ],
      "source": [
        "def validate_and_process_chunk(chunk: str, max_len: int = 250) -> List[str]:\n",
        "    \"\"\"\n",
        "    Clean and validate text chunks, splitting if necessary.\n",
        "    Based on your original validate_and_process_chunk function.\n",
        "    \"\"\"\n",
        "    # Remove URLs and clean the chunk\n",
        "    cleaned_chunk = re.sub(r\"https?://\\S+\", \"\", chunk).strip()\n",
        "\n",
        "    if not cleaned_chunk:\n",
        "        return []\n",
        "\n",
        "    if len(cleaned_chunk) <= max_len:\n",
        "        return [cleaned_chunk]\n",
        "\n",
        "    # Split long chunks at word boundaries\n",
        "    sub_chunks = []\n",
        "    while len(cleaned_chunk) > max_len:\n",
        "        split_pos = cleaned_chunk.rfind(\" \", 0, max_len)\n",
        "        if split_pos == -1:\n",
        "            split_pos = max_len\n",
        "        sub_chunks.append(cleaned_chunk[:split_pos])\n",
        "        cleaned_chunk = cleaned_chunk[split_pos:].lstrip()\n",
        "\n",
        "    if cleaned_chunk:\n",
        "        sub_chunks.append(cleaned_chunk)\n",
        "\n",
        "    return sub_chunks\n",
        "\n",
        "def validate_token_length(text: str, tokenizer, token_limit: int = XTTS_TOKEN_LIMIT) -> Tuple[bool, int]:\n",
        "    \"\"\"\n",
        "    Validate if text is within the token limit for XTTS model.\n",
        "    Returns (is_valid, token_count)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokens = tokenizer.encode(text, lang=\"en\")\n",
        "        return len(tokens) <= token_limit, len(tokens)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not tokenize text: {e}\")\n",
        "        return False, 0\n",
        "\n",
        "def process_text_content(text_content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Process full text content into valid chunks for TTS generation.\n",
        "    Based on your original text processing logic.\n",
        "    \"\"\"\n",
        "    print(\"üìù Processing text content into chunks...\")\n",
        "\n",
        "    # Split into paragraphs and then sentences\n",
        "    paragraphs = text_content.split(\"\\n\")\n",
        "    initial_chunks = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if paragraph.strip():\n",
        "            # Use TTS client's sentence splitter\n",
        "            sentences = tts_client.synthesizer.split_into_sentences(paragraph)\n",
        "            initial_chunks.extend(sentences)\n",
        "\n",
        "    # Further process chunks to ensure they're within limits\n",
        "    final_chunks = []\n",
        "    for chunk in initial_chunks:\n",
        "        sub_chunks = validate_and_process_chunk(chunk)\n",
        "        final_chunks.extend(sub_chunks)\n",
        "\n",
        "    # Validate token lengths and filter out invalid chunks\n",
        "    valid_chunks = []\n",
        "    skipped_count = 0\n",
        "\n",
        "    for i, chunk in enumerate(final_chunks):\n",
        "        is_valid, token_count = validate_token_length(chunk, tokenizer)\n",
        "\n",
        "        if is_valid:\n",
        "            valid_chunks.append(chunk)\n",
        "        else:\n",
        "            skipped_count += 1\n",
        "            logger.warning(f\"SKIPPING CHUNK {i+1}: Too long ({token_count} tokens > {XTTS_TOKEN_LIMIT})\")\n",
        "            logger.warning(f\"Content preview: '{chunk[:80]}...'\")\n",
        "\n",
        "    print(f\"‚úÖ Processed {len(final_chunks)} chunks\")\n",
        "    print(f\"‚úÖ Valid chunks: {len(valid_chunks)}\")\n",
        "    if skipped_count > 0:\n",
        "        print(f\"‚ö†Ô∏è Skipped chunks: {skipped_count}\")\n",
        "\n",
        "    return valid_chunks\n",
        "\n",
        "# Test the processing functions\n",
        "print(\"üß™ Text processing functions defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d983546d",
      "metadata": {
        "id": "d983546d"
      },
      "source": [
        "## 5. Create Batch Audio Generation Functions\n",
        "\n",
        "Implement batch processing functions for parallel audio generation with configurable batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "215e24b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "215e24b8",
        "outputId": "7cd4c1bb-6fc8-47f2-d98a-5ab860716ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Batch Processing Configuration:\n",
            "   Optimal batch size: 8\n",
            "   Max concurrent workers: 4\n",
            "   GPU Memory: 14.7 GB\n",
            "üìà Performance tracking initialized\n"
          ]
        }
      ],
      "source": [
        "def calculate_optimal_batch_size(gpu_memory_gb: float, device: str) -> int:\n",
        "    \"\"\"\n",
        "    Calculate optimal batch size based on available GPU memory.\n",
        "    \"\"\"\n",
        "    if device == \"cpu\":\n",
        "        return 1  # No batching for CPU\n",
        "\n",
        "    # Conservative estimates for XTTS v2 memory usage\n",
        "    if gpu_memory_gb >= 12:\n",
        "        return 8  # High-end GPUs\n",
        "    elif gpu_memory_gb >= 8:\n",
        "        return 6  # Mid-range GPUs\n",
        "    elif gpu_memory_gb >= 6:\n",
        "        return 4  # Entry-level GPUs\n",
        "    else:\n",
        "        return 2  # Low memory GPUs\n",
        "\n",
        "def generate_audio_batch(text_chunks: List[str], batch_id: int, speaker: str = DEFAULT_SPEAKER) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generate audio for a batch of text chunks.\n",
        "    Returns list of audio arrays.\n",
        "    \"\"\"\n",
        "    audio_arrays = []\n",
        "\n",
        "    for i, text in enumerate(text_chunks):\n",
        "        try:\n",
        "            # Generate audio for single chunk\n",
        "            wav_chunk = tts_client.tts(text=text, language=\"en\", speaker=speaker)\n",
        "            audio_arrays.append(np.array(wav_chunk))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating audio for chunk {i} in batch {batch_id}: {e}\")\n",
        "            # Add silence for failed chunks to maintain sequence\n",
        "            silence = np.zeros(int(22050 * 0.5))  # 0.5 seconds of silence at 22050Hz\n",
        "            audio_arrays.append(silence)\n",
        "\n",
        "    return audio_arrays\n",
        "\n",
        "def create_batches(items: List, batch_size: int) -> List[List]:\n",
        "    \"\"\"\n",
        "    Split a list into batches of specified size.\n",
        "    \"\"\"\n",
        "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n",
        "\n",
        "# Configuration for batch processing\n",
        "OPTIMAL_BATCH_SIZE = calculate_optimal_batch_size(gpu_memory, device)\n",
        "MAX_WORKERS = min(4, OPTIMAL_BATCH_SIZE)  # Limit concurrent workers\n",
        "\n",
        "print(f\"‚öôÔ∏è Batch Processing Configuration:\")\n",
        "print(f\"   Optimal batch size: {OPTIMAL_BATCH_SIZE}\")\n",
        "print(f\"   Max concurrent workers: {MAX_WORKERS}\")\n",
        "print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "# Performance tracking\n",
        "class PerformanceTracker:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.chunk_times = []\n",
        "        self.batch_times = []\n",
        "\n",
        "    def start(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_chunk(self):\n",
        "        if self.start_time:\n",
        "            self.chunk_times.append(time.time() - self.start_time)\n",
        "\n",
        "    def log_batch(self, batch_size):\n",
        "        if self.start_time:\n",
        "            batch_time = time.time() - self.start_time\n",
        "            self.batch_times.append((batch_time, batch_size))\n",
        "            self.start_time = time.time()  # Reset for next batch\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.chunk_times:\n",
        "            return \"No performance data available\"\n",
        "\n",
        "        avg_chunk_time = sum(self.chunk_times) / len(self.chunk_times)\n",
        "        total_chunks = len(self.chunk_times)\n",
        "\n",
        "        stats = f\"üìä Performance Stats:\\n\"\n",
        "        stats += f\"   Total chunks: {total_chunks}\\n\"\n",
        "        stats += f\"   Average time per chunk: {avg_chunk_time:.2f}s\\n\"\n",
        "\n",
        "        if self.batch_times:\n",
        "            total_batch_time = sum(t for t, _ in self.batch_times)\n",
        "            total_batch_chunks = sum(s for _, s in self.batch_times)\n",
        "            stats += f\"   Total processing time: {total_batch_time:.2f}s\\n\"\n",
        "            stats += f\"   Throughput: {total_batch_chunks/total_batch_time:.2f} chunks/sec\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "perf_tracker = PerformanceTracker()\n",
        "print(\"üìà Performance tracking initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2f59e4",
      "metadata": {
        "id": "bc2f59e4"
      },
      "source": [
        "## 6. Implement GPU Memory Management\n",
        "\n",
        "Add GPU memory monitoring, automatic cleanup, and fallback mechanisms for memory overflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23f6c18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23f6c18e",
        "outputId": "467b9819-45cc-44c1-b446-82267c139842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† GPU Memory Manager initialized\n",
            "Initial üíæ GPU Memory: 1.79GB/14.74GB (12.1%)\n"
          ]
        }
      ],
      "source": [
        "class GPUMemoryManager:\n",
        "    \"\"\"\n",
        "    Manages GPU memory with monitoring and automatic cleanup.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: str):\n",
        "        self.device = device\n",
        "        self.memory_threshold = 0.9  # 90% memory usage threshold\n",
        "        self.cleanup_threshold = 0.95  # 95% triggers aggressive cleanup\n",
        "\n",
        "    def get_memory_info(self) -> Dict[str, float]:\n",
        "        \"\"\"Get current GPU memory usage information.\"\"\"\n",
        "        if self.device == \"cpu\":\n",
        "            return {\"allocated\": 0, \"reserved\": 0, \"free\": 100, \"used_percent\": 0}\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated(0) / (1024**3)  # GB\n",
        "        reserved = torch.cuda.memory_reserved(0) / (1024**3)   # GB\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "        free = total - allocated\n",
        "        used_percent = allocated / total\n",
        "\n",
        "        return {\n",
        "            \"allocated\": allocated,\n",
        "            \"reserved\": reserved,\n",
        "            \"total\": total,\n",
        "            \"free\": free,\n",
        "            \"used_percent\": used_percent\n",
        "        }\n",
        "\n",
        "    def print_memory_status(self, prefix: str = \"\"):\n",
        "        \"\"\"Print current memory status.\"\"\"\n",
        "        if self.device == \"cpu\":\n",
        "            print(f\"{prefix}üíæ CPU Mode - No GPU memory tracking\")\n",
        "            return\n",
        "\n",
        "        info = self.get_memory_info()\n",
        "        print(f\"{prefix}üíæ GPU Memory: {info['allocated']:.2f}GB/{info['total']:.2f}GB ({info['used_percent']*100:.1f}%)\")\n",
        "\n",
        "    def cleanup_memory(self, aggressive: bool = False):\n",
        "        \"\"\"Clean up GPU memory.\"\"\"\n",
        "        if self.device == \"cpu\":\n",
        "            return\n",
        "\n",
        "        if aggressive:\n",
        "            # Aggressive cleanup\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            print(\"üßπ Aggressive GPU memory cleanup completed\")\n",
        "        else:\n",
        "            # Light cleanup\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def check_memory_and_cleanup(self) -> bool:\n",
        "        \"\"\"Check memory usage and cleanup if needed. Returns True if memory is OK.\"\"\"\n",
        "        if self.device == \"cpu\":\n",
        "            return True\n",
        "\n",
        "        info = self.get_memory_info()\n",
        "\n",
        "        if info['used_percent'] > self.cleanup_threshold:\n",
        "            print(f\"‚ö†Ô∏è High memory usage ({info['used_percent']*100:.1f}%), performing aggressive cleanup...\")\n",
        "            self.cleanup_memory(aggressive=True)\n",
        "            return False\n",
        "        elif info['used_percent'] > self.memory_threshold:\n",
        "            print(f\"‚ö†Ô∏è Memory usage high ({info['used_percent']*100:.1f}%), performing light cleanup...\")\n",
        "            self.cleanup_memory(aggressive=False)\n",
        "            return True\n",
        "\n",
        "        return True\n",
        "\n",
        "    def monitor_memory_during_batch(self, batch_id: int, batch_size: int):\n",
        "        \"\"\"Monitor memory during batch processing.\"\"\"\n",
        "        info = self.get_memory_info()\n",
        "        if info['used_percent'] > 0.8:  # 80% threshold for warnings\n",
        "            print(f\"‚ö†Ô∏è Batch {batch_id}: High memory usage {info['used_percent']*100:.1f}%\")\n",
        "\n",
        "# Initialize memory manager\n",
        "memory_manager = GPUMemoryManager(device)\n",
        "print(\"üß† GPU Memory Manager initialized\")\n",
        "memory_manager.print_memory_status(\"Initial \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95eac13c",
      "metadata": {
        "id": "95eac13c"
      },
      "source": [
        "## 7. Execute Parallel TTS Generation\n",
        "\n",
        "Run the main TTS generation with progress tracking, error handling, and parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "18402aef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18402aef",
        "outputId": "a9cdf313-9f99-45d7-c995-dab193d5d40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Auto-selected: digest_2025-10-03_cleaned.txt\n",
            "\\nüìñ Loading text from: digest_2025-10-03_cleaned.txt\n",
            "‚úÖ Loaded 69135 characters\n",
            "üìä Text preview: Newsletter from: The AI Report.\n",
            "\n",
            "WORK WITH US ‚Ä¢ COMMUNITY ‚Ä¢ PODCASTS ‚Ä¢ B2B TRAINING\n",
            "\n",
            "\n",
            "\n",
            "----------\n",
            "\n",
            "View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,qual...\n"
          ]
        }
      ],
      "source": [
        "# Select and load input text file\n",
        "if input_files:\n",
        "    if len(input_files) == 1:\n",
        "        selected_file = input_files[0]\n",
        "        print(f\"üìÑ Auto-selected: {selected_file}\")\n",
        "    else:\n",
        "        print(\"\\\\nüìã Multiple text files found. Please select one:\")\n",
        "        for i, file in enumerate(input_files, 1):\n",
        "            print(f\"   {i}. {file}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                choice = int(input(\"Enter file number: \")) - 1\n",
        "                if 0 <= choice < len(input_files):\n",
        "                    selected_file = input_files[choice]\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Invalid choice. Please try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid number.\")\n",
        "\n",
        "    # Load the selected file\n",
        "    input_file_path = os.path.join(INPUT_FOLDER, selected_file)\n",
        "\n",
        "    print(f\"\\\\nüìñ Loading text from: {selected_file}\")\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            text_content = f.read().strip()\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(text_content)} characters\")\n",
        "        print(f\"üìä Text preview: {text_content[:200]}{'...' if len(text_content) > 200 else ''}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading file: {e}\")\n",
        "        raise\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No text files found! Please upload a text file to the TTS_Input folder.\")\n",
        "    raise FileNotFoundError(\"No input text files available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4cefa1ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "589e5df82a47490e954c25b86a225ca3",
            "d705478398ae4d6c955af583618e8286",
            "ed0529017799470d8954cb8ada5a89e6",
            "9de2c41673af46a992ce95110606623b",
            "f10085a6e7334502b9535d8a3ad26d0a",
            "3c521f222ec34440b9bcbf7b43c7d78f",
            "1b817fd6f1d747fca23ee203b34bc768",
            "c831806968524da59cd234f31b251ebf",
            "724a0a3685264f93af530fde5a85f79f",
            "35a8ff80f5014a4b86852fe18ea20dac",
            "85aef35ace674f338a02282701139d28"
          ]
        },
        "id": "4cefa1ab",
        "outputId": "3c1c65ec-8d77-4d14-8166-0c4523cfd206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nüîÑ Processing text into TTS-ready chunks...\n",
            "üìù Processing text content into chunks...\n",
            "‚úÖ Processed 849 chunks\n",
            "‚úÖ Valid chunks: 849\n",
            "‚úÖ Generated 849 valid chunks for processing\n",
            "üì¶ Created 107 batches (batch size: 8)\n",
            "\\nüéµ Starting TTS generation...\n",
            "   Total chunks: 849\n",
            "   Batch size: 8\n",
            "   Total batches: 107\n",
            "   Device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Audio:   0%|          | 0/849 [00:00<?, ?chunk/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "589e5df82a47490e954c25b86a225ca3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n‚úÖ TTS Generation Complete!\n",
            "   Successful chunks: 849\n",
            "   Failed chunks: 0\n",
            "   Total audio segments: 849\n",
            "üßπ Aggressive GPU memory cleanup completed\n",
            "Final üíæ GPU Memory: 1.79GB/14.74GB (12.1%)\n"
          ]
        }
      ],
      "source": [
        "# Process text into valid chunks\n",
        "print(\"\\\\nüîÑ Processing text into TTS-ready chunks...\")\n",
        "text_chunks = process_text_content(text_content)\n",
        "\n",
        "if not text_chunks:\n",
        "    print(\"‚ùå No valid text chunks generated!\")\n",
        "    raise ValueError(\"Text processing failed - no valid chunks\")\n",
        "\n",
        "print(f\"‚úÖ Generated {len(text_chunks)} valid chunks for processing\")\n",
        "\n",
        "# Create batches for parallel processing\n",
        "batches = create_batches(text_chunks, OPTIMAL_BATCH_SIZE)\n",
        "print(f\"üì¶ Created {len(batches)} batches (batch size: {OPTIMAL_BATCH_SIZE})\")\n",
        "\n",
        "# Initialize tracking\n",
        "perf_tracker.start()\n",
        "all_audio_chunks = []\n",
        "successful_chunks = 0\n",
        "failed_chunks = 0\n",
        "\n",
        "print(f\"\\\\nüéµ Starting TTS generation...\")\n",
        "print(f\"   Total chunks: {len(text_chunks)}\")\n",
        "print(f\"   Batch size: {OPTIMAL_BATCH_SIZE}\")\n",
        "print(f\"   Total batches: {len(batches)}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Main processing loop with progress bar\n",
        "with tqdm(total=len(text_chunks), desc=\"Generating Audio\", unit=\"chunk\") as pbar:\n",
        "\n",
        "    for batch_id, batch_chunks in enumerate(batches, 1):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Memory check before processing batch\n",
        "        memory_manager.check_memory_and_cleanup()\n",
        "        memory_manager.monitor_memory_during_batch(batch_id, len(batch_chunks))\n",
        "\n",
        "        try:\n",
        "            # Process batch\n",
        "            pbar.set_description(f\"Processing Batch {batch_id}/{len(batches)}\")\n",
        "\n",
        "            batch_audio = generate_audio_batch(batch_chunks, batch_id, DEFAULT_SPEAKER)\n",
        "\n",
        "            # Verify batch results\n",
        "            if len(batch_audio) == len(batch_chunks):\n",
        "                all_audio_chunks.extend(batch_audio)\n",
        "                successful_chunks += len(batch_chunks)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Batch {batch_id}: Expected {len(batch_chunks)} audio chunks, got {len(batch_audio)}\")\n",
        "                all_audio_chunks.extend(batch_audio)\n",
        "                successful_chunks += len(batch_audio)\n",
        "                failed_chunks += len(batch_chunks) - len(batch_audio)\n",
        "\n",
        "            # Update progress\n",
        "            pbar.update(len(batch_chunks))\n",
        "\n",
        "            # Performance tracking\n",
        "            batch_time = time.time() - batch_start_time\n",
        "            perf_tracker.log_batch(len(batch_chunks))\n",
        "\n",
        "            # Memory cleanup after batch\n",
        "            if batch_id % 3 == 0:  # Cleanup every 3 batches\n",
        "                memory_manager.cleanup_memory(aggressive=False)\n",
        "\n",
        "            # Progress update\n",
        "            chunks_per_sec = len(batch_chunks) / batch_time\n",
        "            pbar.set_postfix({\n",
        "                'chunks/sec': f'{chunks_per_sec:.1f}',\n",
        "                'batch_time': f'{batch_time:.1f}s',\n",
        "                'success': successful_chunks,\n",
        "                'failed': failed_chunks\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\\\n‚ùå Error in batch {batch_id}: {e}\")\n",
        "            failed_chunks += len(batch_chunks)\n",
        "            pbar.update(len(batch_chunks))\n",
        "\n",
        "            # Add silence for failed batch to maintain sequence\n",
        "            silence_duration = 22050 * 2  # 2 seconds of silence\n",
        "            for _ in batch_chunks:\n",
        "                silence = np.zeros(silence_duration)\n",
        "                all_audio_chunks.append(silence)\n",
        "\n",
        "print(f\"\\\\n‚úÖ TTS Generation Complete!\")\n",
        "print(f\"   Successful chunks: {successful_chunks}\")\n",
        "print(f\"   Failed chunks: {failed_chunks}\")\n",
        "print(f\"   Total audio segments: {len(all_audio_chunks)}\")\n",
        "\n",
        "# Final memory cleanup\n",
        "memory_manager.cleanup_memory(aggressive=True)\n",
        "memory_manager.print_memory_status(\"Final \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4938fa41",
      "metadata": {
        "id": "4938fa41"
      },
      "source": [
        "## 8. Save and Export Audio Files\n",
        "\n",
        "Concatenate all audio chunks, export to WAV/MP3 format, and save to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b11ebe6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "b11ebe6c",
        "outputId": "8ff1802f-7df2-4c12-a8db-e0d482ca5c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Concatenating audio chunks...\n",
            "üíæ Saving audio to: /content/drive/MyDrive/TTS_Output/digest_2025-10-03_cleaned_generated_audio.wav\n",
            "üéµ Converting to MP3 format...\n",
            "üîß Optimizing audio format for smaller file size...\n",
            "   ‚ÑπÔ∏è High sample rate detected: 24000 Hz\n",
            "\n",
            "üéâ Audio generation completed successfully!\n",
            "   üìÅ WAV file: /content/drive/MyDrive/TTS_Output/digest_2025-10-03_cleaned_generated_audio.wav\n",
            "   üìÅ MP3 file: /content/drive/MyDrive/TTS_Output/digest_2025-10-03_cleaned_generated_audio.mp3\n",
            "   ‚è±Ô∏è Duration: 69.8 minutes (4189.3 seconds)\n",
            "   üìä WAV size: 191.77 MB\n",
            "   üìä MP3 size: 15.98 MB (bitrate: 64k)\n",
            "   üîä Sample rate: 24000 Hz\n",
            "   üì∫ Channels: 1\n",
            "   üéµ Bit depth: 16-bit\n",
            "\n",
            "No performance data available\n",
            "\n",
            "‚ö†Ô∏è MP3 size (15.98 MB) exceeds upload limit (15.0 MB)\n",
            "   Your local script will automatically split this into chunks during upload.\n",
            "\n",
            "üì• Download Instructions:\n",
            "   1. Navigate to the TTS_Output folder in your Google Drive\n",
            "   2. Download the MP3 file: digest_2025-10-03_cleaned_generated_audio.mp3\n",
            "   3. Save it to your local archive_mp3 folder\n",
            "   4. Run the upload step in your local workflow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b882334-3ade-45ba-8dba-166d59f7a04f\", \"digest_2025-10-03_cleaned_generated_audio.mp3\", 16757805)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Download started!\n",
            "\n",
            "üèÅ TTS processing workflow complete!\n"
          ]
        }
      ],
      "source": [
        "if all_audio_chunks:\n",
        "    print(\"üîó Concatenating audio chunks...\")\n",
        "\n",
        "    # Concatenate all audio chunks\n",
        "    full_audio_np = np.concatenate(all_audio_chunks)\n",
        "\n",
        "    # Generate output filename based on input filename\n",
        "    base_name = os.path.splitext(selected_file)[0]\n",
        "    output_filename = f\"{base_name}_generated_audio\"\n",
        "\n",
        "    # Save as WAV first (higher quality, compatible with your existing workflow)\n",
        "    wav_output_path = os.path.join(OUTPUT_FOLDER, f\"{output_filename}.wav\")\n",
        "\n",
        "    print(f\"üíæ Saving audio to: {wav_output_path}\")\n",
        "\n",
        "    try:\n",
        "        # Use TTS client's save function for consistency\n",
        "        tts_client.synthesizer.save_wav(wav=full_audio_np, path=wav_output_path)\n",
        "\n",
        "        # Get file size and duration info\n",
        "        wav_size_mb = os.path.getsize(wav_output_path) / (1024 * 1024)\n",
        "\n",
        "        # Load with pydub to get duration and convert to MP3\n",
        "        print(\"üéµ Converting to MP3 format...\")\n",
        "        audio_segment = AudioSegment.from_wav(wav_output_path)\n",
        "        duration_seconds = len(audio_segment) / 1000.0\n",
        "\n",
        "        # üîß OPTIMIZED: Convert to 16-bit and lower sample rate if needed\n",
        "        print(\"üîß Optimizing audio format for smaller file size...\")\n",
        "\n",
        "        # Convert to 16-bit if it's higher bit depth\n",
        "        if audio_segment.sample_width > 2:  # More than 16-bit\n",
        "            audio_segment = audio_segment.set_sample_width(2)  # 16-bit\n",
        "            print(f\"   ‚úÖ Converted to 16-bit (was {audio_segment.sample_width * 8}-bit)\")\n",
        "\n",
        "        # Optionally downsample if sample rate is very high\n",
        "        if audio_segment.frame_rate > 22050:\n",
        "            print(f\"   ‚ÑπÔ∏è High sample rate detected: {audio_segment.frame_rate} Hz\")\n",
        "            # Uncomment next line if you want to downsample to 22050 Hz\n",
        "            # audio_segment = audio_segment.set_frame_rate(22050)\n",
        "            # print(f\"   ‚úÖ Downsampled to 22050 Hz\")\n",
        "\n",
        "        # Export MP3 version with configurable bitrate\n",
        "        mp3_output_path = os.path.join(OUTPUT_FOLDER, f\"{output_filename}.mp3\")\n",
        "\n",
        "        # üéØ ADJUSTABLE BITRATE: Change this to match your previous file sizes\n",
        "        # For ~18MB/60min (like your previous results), use 48k or 64k\n",
        "        TARGET_BITRATE = \"64k\"  # Options: \"48k\", \"64k\", \"96k\", \"128k\"\n",
        "\n",
        "        audio_segment.export(mp3_output_path, format=\"mp3\") # , bitrate=TARGET_BITRATE\n",
        "\n",
        "        mp3_size_mb = os.path.getsize(mp3_output_path) / (1024 * 1024)\n",
        "\n",
        "        print(f\"\\nüéâ Audio generation completed successfully!\")\n",
        "        print(f\"   üìÅ WAV file: {wav_output_path}\")\n",
        "        print(f\"   üìÅ MP3 file: {mp3_output_path}\")\n",
        "        print(f\"   ‚è±Ô∏è Duration: {duration_seconds/60:.1f} minutes ({duration_seconds:.1f} seconds)\")\n",
        "        print(f\"   üìä WAV size: {wav_size_mb:.2f} MB\")\n",
        "        print(f\"   üìä MP3 size: {mp3_size_mb:.2f} MB (bitrate: {TARGET_BITRATE})\")\n",
        "        print(f\"   üîä Sample rate: {audio_segment.frame_rate} Hz\")\n",
        "        print(f\"   üì∫ Channels: {audio_segment.channels}\")\n",
        "        print(f\"   üéµ Bit depth: {audio_segment.sample_width * 8}-bit\")\n",
        "\n",
        "        # Performance summary\n",
        "        print(f\"\\n{perf_tracker.get_stats()}\")\n",
        "\n",
        "        # Check if MP3 size is within upload limits (from your original code)\n",
        "        MAX_UPLOAD_SIZE_MB = 15.0\n",
        "        if mp3_size_mb <= MAX_UPLOAD_SIZE_MB:\n",
        "            print(f\"\\n‚úÖ MP3 size ({mp3_size_mb:.2f} MB) is within upload limit ({MAX_UPLOAD_SIZE_MB} MB)\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è MP3 size ({mp3_size_mb:.2f} MB) exceeds upload limit ({MAX_UPLOAD_SIZE_MB} MB)\")\n",
        "            print(f\"   Your local script will automatically split this into chunks during upload.\")\n",
        "\n",
        "        # Download instructions for Colab users\n",
        "        if IN_COLAB:\n",
        "            print(f\"\\nüì• Download Instructions:\")\n",
        "            print(f\"   1. Navigate to the TTS_Output folder in your Google Drive\")\n",
        "            print(f\"   2. Download the MP3 file: {output_filename}.mp3\")\n",
        "            print(f\"   3. Save it to your local archive_mp3 folder\")\n",
        "            print(f\"   4. Run the upload step in your local workflow\")\n",
        "\n",
        "            try:\n",
        "                files.download(mp3_output_path)\n",
        "                print(\"‚úÖ Download started!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Download failed: {e}\")\n",
        "                print(\"Please download manually from Google Drive\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving audio files: {e}\")\n",
        "        raise\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No audio chunks generated - cannot create output file!\")\n",
        "\n",
        "print(\"\\nüèÅ TTS processing workflow complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77b869d",
      "metadata": {
        "id": "d77b869d"
      },
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "**Your optimized TTS generation is complete!** Here's what to do next:\n",
        "\n",
        "### 1. **Download the Generated Audio**\n",
        "- The MP3 file is saved in your Google Drive `TTS_Output` folder\n",
        "- Download it to your local `archive_mp3` folder\n",
        "\n",
        "### 2. **Continue with Local Upload**\n",
        "- Return to your local environment\n",
        "- Run the upload portion of your workflow\n",
        "- The local script will handle the API upload and metadata\n",
        "\n",
        "### 3. **Performance Benefits**\n",
        "This optimized workflow provides:\n",
        "- **5-20x faster generation** compared to local CPU processing\n",
        "- **Parallel chunk processing** for maximum GPU utilization  \n",
        "- **Smart memory management** to prevent crashes\n",
        "- **Automatic error recovery** for robust processing\n",
        "- **Progress tracking** for real-time feedback\n",
        "\n",
        "### 4. **Troubleshooting**\n",
        "If you encounter issues:\n",
        "- Check the TTS_Output folder in Google Drive\n",
        "- Verify the MP3 file was created successfully\n",
        "- Ensure your local archive_mp3 folder exists\n",
        "- Run your local upload script as normal\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Happy audio generation!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "589e5df82a47490e954c25b86a225ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d705478398ae4d6c955af583618e8286",
              "IPY_MODEL_ed0529017799470d8954cb8ada5a89e6",
              "IPY_MODEL_9de2c41673af46a992ce95110606623b"
            ],
            "layout": "IPY_MODEL_f10085a6e7334502b9535d8a3ad26d0a"
          }
        },
        "d705478398ae4d6c955af583618e8286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c521f222ec34440b9bcbf7b43c7d78f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b817fd6f1d747fca23ee203b34bc768",
            "value": "Processing‚ÄáBatch‚Äá107/107:‚Äá100%"
          }
        },
        "ed0529017799470d8954cb8ada5a89e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c831806968524da59cd234f31b251ebf",
            "max": 849,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_724a0a3685264f93af530fde5a85f79f",
            "value": 849
          }
        },
        "9de2c41673af46a992ce95110606623b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a8ff80f5014a4b86852fe18ea20dac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85aef35ace674f338a02282701139d28",
            "value": "‚Äá849/849‚Äá[47:20&lt;00:00,‚Äá‚Äá3.03s/chunk,‚Äáchunks/sec=1.2,‚Äábatch_time=0.8s,‚Äásuccess=849,‚Äáfailed=0]"
          }
        },
        "f10085a6e7334502b9535d8a3ad26d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c521f222ec34440b9bcbf7b43c7d78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b817fd6f1d747fca23ee203b34bc768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c831806968524da59cd234f31b251ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724a0a3685264f93af530fde5a85f79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35a8ff80f5014a4b86852fe18ea20dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85aef35ace674f338a02282701139d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}